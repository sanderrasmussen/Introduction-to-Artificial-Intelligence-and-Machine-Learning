\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{in3050\_in4050\_2024\_assignment2}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{in3050in4050-mandatory-assignment-2-2024-supervised-learning}{%
\subsection{IN3050/IN4050 Mandatory Assignment 2, 2024: Supervised
Learning}\label{in3050in4050-mandatory-assignment-2-2024-supervised-learning}}

    \hypertarget{rules}{%
\subsubsection{Rules}\label{rules}}

Before you begin the exercise, review the rules at this website:
https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html
, in particular the paragraph on cooperation. This is an individual
assignment. You are not allowed to deliver together or copy/share
source-code/answers with others. Read also the ``Routines for handling
suspicion of cheating and attempted cheating at the University of
Oslo'':
https://www.uio.no/english/studies/examinations/cheating/index.html By
submitting this assignment, you confirm that you are familiar with the
rules and the consequences of breaking them.

\hypertarget{delivery}{%
\subsubsection{Delivery}\label{delivery}}

\textbf{Deadline}: Friday, March 22, 2024, 23:59

Your submission should be delivered in Devilry. You may redeliver in
Devilry before the deadline, but include all files in the last delivery,
as only the last delivery will be read. You are recommended to upload
preliminary versions hours (or days) before the final deadline.

\hypertarget{what-to-deliver}{%
\subsubsection{What to deliver?}\label{what-to-deliver}}

You are recommended to solve the exercise in a Jupyter notebook, but you
might solve it in a regular Python script if you prefer.

\hypertarget{alternative-1}{%
\paragraph{Alternative 1}\label{alternative-1}}

If you prefer not to use notebooks, you should deliver the code, your
run results, and a PDF report where you answer all the questions and
explain your work.

\hypertarget{alternative-2}{%
\paragraph{Alternative 2}\label{alternative-2}}

If you choose Jupyter, you should deliver the notebook. You should
answer all questions and explain what you are doing in Markdown. Still,
the code should be properly commented. The notebook should contain
results of your runs. In addition, you should make a pdf of your
solution which shows the results of the runs. (If you can't export:
notebook -\textgreater{} latex -\textgreater{} pdf on your own machine,
you may do this on the IFI linux machines.)

Here is a list of \emph{absolutely necessary} (but not sufficient)
conditions to get the assignment marked as passed:

\begin{itemize}
\tightlist
\item
  You must deliver your code (Python script or Jupyter notebook) you
  used to solve the assignment.
\item
  The code used for making the output and plots must be included in the
  assignment.
\item
  You must include example runs that clearly shows how to run all
  implemented functions and methods.
\item
  All the code (in notebook cells or python main-blocks) must run. If
  you have unfinished code that crashes, please comment it out and
  document what you think causes it to crash.
\item
  You must also deliver a pdf of the code, outputs, comments and plots
  as explained above.
\end{itemize}

Your report/notebook should contain your name and username.

Deliver one single compressed folder (.zip, .tgz or .tar.gz) which
contains your complete solution.

Important: if you weren't able to finish the assignment, use the PDF
report/Markdown to elaborate on what you've tried and what problems you
encountered. Students who have made an effort and attempted all parts of
the assignment will get a second chance even if they fail initially.
This exercise will be graded PASS/FAIL.

    \hypertarget{goals-of-the-assignment}{%
\subsubsection{Goals of the assignment}\label{goals-of-the-assignment}}

The goal of this assignment is to get a better understanding of
supervised learning with gradient descent. It will, in particular,
consider the similarities and differences between linear classifiers and
multi-layer feed forward neural networks (multi-layer perceptrons, MLP)
and the differences and similarities between binary and multi-class
classification. A significant part is dedicated to implementing and
understanding the backpropagation algorithm.

\hypertarget{tools}{%
\subsubsection{Tools}\label{tools}}

The aim of the exercises is to give you a look inside the learning
algorithms. You may freely use code from the weekly exercises and the
published solutions. You should not use machine learning libraries like
Scikit-Learn or PyTorch, because the point of this assignment is for you
to implement things from scratch. You, however, are encouraged to use
tools like NumPy and Pandas, which are not ML-specific.

The given precode uses NumPy. You are recommended to use NumPy since it
results in more compact code, but feel free to use pure Python if you
prefer.

\hypertarget{beware}{%
\subsubsection{Beware}\label{beware}}

This is a revised assignment compared to earlier years. If anything is
unclear, do not hesitate to ask. Also, if you think some assumptions are
missing, make your own and explain them!

    \hypertarget{initialization}{%
\subsubsection{Initialization}\label{initialization}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{426}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{sklearn} \PY{c+c1}{\PYZsh{} This is only to generate a dataset}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{precision\PYZus{}recall\PYZus{}fscore\PYZus{}support}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

We start by making a synthetic dataset of 2000 instances and five
classes, with 400 instances in each class. (See
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make\_blobs.html
regarding how the data are generated.) We choose to use a synthetic
dataset---and not a set of natural occuring data---because we are mostly
interested in properties of the various learning algorithms, in
particular the differences between linear classifiers and multi-layer
neural networks together with the difference between binary and
multi-class data. In addition, we would like a dataset with instances
represented with only two numerical features, so that it is easy to
visualize the data. It would be rather difficult (although not
impossible) to find a real-world dataset of the same nature. Anyway, you
surely can use the code in this assignment for training machine learning
models on real-world datasets.

When we are doing experiments in supervised learning, and the data are
not already split into training and test sets, we should start by
splitting the data. Sometimes there are natural ways to split the data,
say training on data from one year and testing on data from a later
year, but if that is not the case, we should shuffle the data randomly
before splitting. (OK, that is not necessary with this particular
synthetic data set, since it is already shuffled by default by
Scikit-Learn, but that will not be the case with real-world data) We
should split the data so that we keep the alignment between X (features)
and t (class labels), which may be achieved by shuffling the indices. We
split into 50\% for training, 25\% for validation, and 25\% for final
testing. The set for final testing \emph{must not be used} till the end
of the assignment in part 3.

We fix the seed both for data set generation and for shuffling, so that
we work on the same datasets when we rerun the experiments. This is done
by the \texttt{random\_state} argument and the
\texttt{rng\ =\ np.random.RandomState(2024)}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{427}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Generating the dataset}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}blobs}
\PY{n}{X}\PY{p}{,} \PY{n}{t\PYZus{}multi} \PY{o}{=} \PY{n}{make\PYZus{}blobs}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{centers}\PY{o}{=}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} 
                  \PY{n}{n\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{2024}\PY{p}{,} \PY{n}{cluster\PYZus{}std}\PY{o}{=}\PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{428}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Shuffling the dataset}
\PY{n}{indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{rng} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{2024}\PY{p}{)}
\PY{n}{rng}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{indices}\PY{p}{)}
\PY{n}{indices}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{428}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([ 937, 1776,  868, 1282, 1396,  147,  601, 1193, 1789,  547])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{429}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Splitting into train, dev and test}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{]}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{:}\PY{l+m+mi}{1500}\PY{p}{]}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{l+m+mi}{1500}\PY{p}{:}\PY{p}{]}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\PY{n}{t\PYZus{}multi\PYZus{}train} \PY{o}{=} \PY{n}{t\PYZus{}multi}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{]}\PY{p}{]}
\PY{n}{t\PYZus{}multi\PYZus{}val} \PY{o}{=} \PY{n}{t\PYZus{}multi}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{:}\PY{l+m+mi}{1500}\PY{p}{]}\PY{p}{]}
\PY{n}{t\PYZus{}multi\PYZus{}test} \PY{o}{=} \PY{n}{t\PYZus{}multi}\PY{p}{[}\PY{n}{indices}\PY{p}{[}\PY{l+m+mi}{1500}\PY{p}{:}\PY{p}{]}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Next, we will make a second dataset with only two classes by merging the
existing labels in (X,t), so that \texttt{0}, \texttt{1} and \texttt{2}
become the new \texttt{0} and \texttt{3} and \texttt{4} become the new
\texttt{1}. Let's call the new set (X, t2). This will be a binary set.
We now have two datasets:

\begin{itemize}
\tightlist
\item
  Binary set: \texttt{(X,\ t2)}
\item
  Multi-class set: \texttt{(X,\ t\_multi)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{430}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{t2\PYZus{}train} \PY{o}{=} \PY{n}{t\PYZus{}multi\PYZus{}train} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{t2\PYZus{}train} \PY{o}{=} \PY{n}{t2\PYZus{}train}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{t2\PYZus{}val} \PY{o}{=} \PY{p}{(}\PY{n}{t\PYZus{}multi\PYZus{}val} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{3}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{t2\PYZus{}test} \PY{o}{=} \PY{p}{(}\PY{n}{t\PYZus{}multi\PYZus{}test} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{3}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    We can plot the two traning sets.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{431}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} You may adjust the size}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{t\PYZus{}multi\PYZus{}train}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mf}{10.0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Multi\PYZhy{}class set}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{431}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, 'Multi-class set')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{432}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mf}{10.0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Binary set}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{432}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, 'Binary set')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{part-1-linear-classifiers}{%
\section{Part 1: Linear classifiers}\label{part-1-linear-classifiers}}

\hypertarget{linear-regression}{%
\subsubsection{Linear regression}\label{linear-regression}}

    We see that even the binary set (X, t2) is far from linearly separable,
and we will explore how various classifiers are able to handle this. We
start with linear regression with the Mean Squared Error (MSE) loss,
although it is not the most widely used approach for classification
tasks: but we are interested. You may make your own implementation from
scratch or start with the solution to the weekly exercise set 7. We
include it here with a little added flexibility.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{433}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{bias}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}X is a NxM matrix: N datapoints, M features}
\PY{l+s+sd}{    bias is a bias term, \PYZhy{}1 or 1, or any other scalar. Use 0 for no bias}
\PY{l+s+sd}{    Return a Nx(M+1) matrix with added bias in position zero}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{N} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{biases} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{n}{bias} \PY{c+c1}{\PYZsh{} Make a N*1 matrix of biases}
    \PY{c+c1}{\PYZsh{} Concatenate the column of biases in front of the columns of X.}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{biases}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{axis}  \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{434}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{NumpyClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Common methods to all Numpy classifiers \PYZhy{}\PYZhy{}\PYZhy{} if any\PYZdq{}\PYZdq{}\PYZdq{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{435}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{NumpyLinRegClass}\PY{p}{(}\PY{n}{NumpyClassifier}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{o}{=}\PY{n}{bias}
    
    \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}X\PYZus{}train is a NxM matrix, N data points, M features}
\PY{l+s+sd}{        t\PYZus{}train is a vector of length N,}
\PY{l+s+sd}{        the target class values for the training data}
\PY{l+s+sd}{        lr is our learning rate}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{:}
            \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
            
        \PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{M}\PY{p}{)} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape} \PY{c+c1}{\PYZsh{}This line unpacks the shape of the feature matrix X\PYZus{}train into N (number of data points) and M (number of features).}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{M}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} print(\PYZdq{}Epoch\PYZdq{}, epoch)}
            \PY{n}{weights} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{/} \PY{n}{N} \PY{o}{*}  \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{p}{(}\PY{n}{X\PYZus{}train} \PY{o}{@} \PY{n}{weights} \PY{o}{\PYZhy{}} \PY{n}{t\PYZus{}train}\PY{p}{)}      
    
    \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{threshold}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}X is a KxM matrix for some K\PYZgt{}=1}
\PY{l+s+sd}{        predict the value for each point in X\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{:}
            \PY{n}{X} \PY{o}{=} \PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
        \PY{n}{ys} \PY{o}{=} \PY{n}{X} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}
        \PY{k}{return} \PY{n}{ys} \PY{o}{\PYZgt{}} \PY{n}{threshold}

\PY{c+c1}{\PYZsh{} the mse should be outside the class}
\PY{k}{def} \PY{n+nf}{MSE}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    We can train and test a first classifier.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{436}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{accuracy}\PY{p}{(}\PY{n}{predicted}\PY{p}{,} \PY{n}{gold}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{gold}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{437}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cl} \PY{o}{=} \PY{n}{NumpyLinRegClass}\PY{p}{(}\PY{p}{)}
\PY{n}{cl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{(}\PY{n}{cl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{}checking the loss}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{MSE}\PY{p}{(}\PY{n}{cl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on the validation set: 0.76
0.28
    \end{Verbatim}

    The following is a small procedure which plots the data set together
with the decision boundaries. You may modify the colors and the rest of
the graphics as you like. The procedure will also work for multi-class
classifiers

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{438}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{clf}\PY{o}{=}\PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Plot the data set (X,t) together with the decision boundary of the classifier clf\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} The region of the plane to consider determined by X}
    \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    
    \PY{c+c1}{\PYZsh{} Make a prediction of the whole region}
    \PY{n}{h} \PY{o}{=} \PY{l+m+mf}{0.02}  \PY{c+c1}{\PYZsh{} step size in the mesh}
    \PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{xx}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Classify each meshpoint.}
    \PY{n}{Z} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{n}{size}\PY{p}{)} \PY{c+c1}{\PYZsh{} You may adjust this}

    \PY{c+c1}{\PYZsh{} Put the result into a color plot}
    \PY{n}{plt}\PY{o}{.}\PY{n}{contourf}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{cmap} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Paired}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{t}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mf}{10.0}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Paired}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{xx}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{n}{yy}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Decision regions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x0}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{439}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{cl}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_24_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{task-tuning}{%
\subsubsection{Task: Tuning}\label{task-tuning}}

The result is far from impressive. Remember that a classifier which
always chooses the majority class will have an accuracy of 0.6 on this
data set.

Your task is to try various settings for the two training
hyper-parameters, learning rate and the number of epochs, to get the
best accuracy on the validation set.

Report how the accuracy varies with the hyper-parameter settings. It it
not sufficient to give the final hyperparameters. You must also show how
you found then and results for alternative values you tried aout.

When you are satisfied with the result, you may plot the decision
boundaries, as above.

    \hypertarget{task-scaling}{%
\subsubsection{Task: Scaling}\label{task-scaling}}

    We have seen in the lectures that scaling the data may improve training
speed and sometimes the performance.

\begin{itemize}
\tightlist
\item
  Implement a scaler, at least the standard scaler (normalizer), but you
  can also try other techniques
\item
  Scale the data
\item
  Train the model on the scaled data
\item
  Experiment with hyper-parameter settings and see whether you can speed
  up the training.
\item
  Report final hyper-parameter settings and show how you found them.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{440}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{normalizerScalar}\PY{p}{(}\PY{p}{)}\PY{p}{:}

    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{mean} \PY{o}{=}\PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{std}\PY{o}{=} \PY{k+kc}{None}
    
    \PY{k}{def} \PY{n+nf}{transform}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} calculate mean and std for every collumn}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

        \PY{c+c1}{\PYZsh{}this is what i found in the lecture video}
        \PY{n}{normalized\PYZus{}X} \PY{o}{=} \PY{p}{(}\PY{n}{X} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{mean}\PY{p}{)} \PY{o}{/} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{std}
        \PY{k}{return} \PY{n}{normalized\PYZus{}X}

\PY{n}{cl} \PY{o}{=} \PY{n}{NumpyLinRegClass}\PY{p}{(}\PY{p}{)}
\PY{n}{nS} \PY{o}{=} \PY{n}{normalizerScalar}\PY{p}{(}\PY{p}{)}

\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train\PYZus{}nm} \PY{o}{=} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}val\PYZus{}nm} \PY{o}{=} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t2\PYZus{}val}\PY{p}{)}



\PY{n}{cl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train\PYZus{}nm}\PY{p}{,}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{(}\PY{n}{cl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}supposedly the label data should not be scaled}

\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{cl}\PY{p}{)}

    
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on the validation set: 0.618
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{logistic-regression}{%
\subsection{Logistic regression}\label{logistic-regression}}

``\,``\,'' a) You should now implement a logistic regression classifier
similarly to the classifier based on linear regression. You may use the
code from the solution to weekly exercise set week07.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\item
  In addition to the method \texttt{predict()} which predicts a class
  for the data, include a method \texttt{predict\_probability()} which
  predict the probability of the data belonging to the positive class.
\item
  So far, we have not calculated the loss explicitly in the code. Extend
  the code to calculate the loss on the training set for each epoch and
  to store the losses such that the losses can be inspected after
  training. The prefered loss for logistic regression is binary
  cross-entropy, but you can also try mean squared error. The most
  important is that your implementation of the loss corresponds to your
  implementation of the gradient descent. Also, calculate and store
  accuracies after each epoch.
\item
  In addition, extend the \texttt{fit()} method with optional arguments
  for a validation set (X\_val, t\_val). If a validation set is included
  in the call to \texttt{fit()}, calculate the loss and the accuracy for
  the validation set after each epoch.
\item
  The training runs for a number of epochs. We cannot know beforehand
  for how many epochs it is reasonable to run the training. One
  possibility is to run the training until the learning does not improve
  much. Extend the \texttt{fit()} method with two keyword arguments,
  \texttt{tol} (tolerance) and \texttt{n\_epochs\_no\_update} and stop
  training when the loss has not improved with more than \texttt{tol}
  after \texttt{n\_epochs\_no\_update}. A possible default value for
  \texttt{n\_epochs\_no\_update} is 5. Also, add an attribute to the
  classifier which tells us after fitting how many epochs it was trained
  for.
\item
  Train classifiers with various learning rates, and with varying values
  for \texttt{tol} for finding the optimal values. Also consider the
  effect of scaling the data.
\item
  After a succesful training, for your best model, plot both training
  loss and validation loss as functions of the number of epochs in one
  figure, and both training and validation accuracies as functions of
  the number of epochs in another figure. Comment on what you see. Are
  the curves monotone? Is this as expected?{}``\,``\,''
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{441}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{class} \PY{n+nc}{NumpyLogRegClass}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{bias}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{losses\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{accuracies\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{losses\PYZus{}val} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{accuracies\PYZus{}val} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trained\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{:}
            \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
            \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}

        \PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{M}\PY{p}{)} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape} 
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{M}\PY{p}{)}
        
        \PY{n}{best\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{inf}
        \PY{n}{no\PYZus{}update\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}

        \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
            
            \PY{c+c1}{\PYZsh{} Forward propagation for training set}
            \PY{n}{sum\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train} \PY{o}{@} \PY{n}{weights}
            \PY{n}{activation\PYZus{}train} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{logistic}\PY{p}{(}\PY{n}{sum\PYZus{}train}\PY{p}{)}\PY{c+c1}{\PYZsh{} applying activation function sigmoid}

            \PY{c+c1}{\PYZsh{} CROSS ENTROPY LOSS }
            \PY{n}{loss\PYZus{}train} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{t\PYZus{}train} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{activation\PYZus{}train}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{t\PYZus{}train}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{activation\PYZus{}train}\PY{p}{)}\PY{p}{)}
            
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{losses\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss\PYZus{}train}\PY{p}{)}
            \PY{n}{predictions\PYZus{}train} \PY{o}{=} \PY{p}{(}\PY{n}{activation\PYZus{}train} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}threshold 0.5 is used for predicting}
            
            \PY{n}{accuracy\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{predictions\PYZus{}train} \PY{o}{==} \PY{n}{t\PYZus{}train}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{accuracies\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy\PYZus{}train}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Forward propagation for validation set}
            \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{o+ow}{and} \PY{n}{t\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                
                \PY{n}{sum\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val} \PY{o}{@} \PY{n}{weights}
                \PY{n}{activation\PYZus{}val} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{logistic}\PY{p}{(}\PY{n}{sum\PYZus{}val}\PY{p}{)}
                
                \PY{n}{loss\PYZus{}val} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{t\PYZus{}val} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{activation\PYZus{}val}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{t\PYZus{}val}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{activation\PYZus{}val}\PY{p}{)}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{losses\PYZus{}val}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss\PYZus{}val}\PY{p}{)}
                
                \PY{n}{predictions\PYZus{}val} \PY{o}{=} \PY{p}{(}\PY{n}{activation\PYZus{}val} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{n}{accuracy\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{predictions\PYZus{}val} \PY{o}{==} \PY{n}{t\PYZus{}val}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{accuracies\PYZus{}val}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy\PYZus{}val}\PY{p}{)}

            \PY{c+c1}{\PYZsh{}Check stopping condition}
            \PY{k}{if} \PY{n}{loss\PYZus{}train} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}loss} \PY{o}{\PYZhy{}} \PY{n}{tol}\PY{p}{:} 
                \PY{n}{best\PYZus{}loss} \PY{o}{=} \PY{n}{loss\PYZus{}train} 
                \PY{n}{no\PYZus{}update\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{}continue from 0, similar to hill climbing}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{no\PYZus{}update\PYZus{}count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                \PY{k}{if} \PY{n}{no\PYZus{}update\PYZus{}count} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{p}{:} \PY{c+c1}{\PYZsh{}if no more improvments, stop}
                    \PY{k}{break}

            \PY{c+c1}{\PYZsh{}Update weights with gradient descent}
            \PY{n}{gradient} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{p}{(}\PY{n}{activation\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{t\PYZus{}train}\PY{p}{)} \PY{o}{/} \PY{n}{N}
            \PY{n}{weights} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{n}{gradient}

            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trained\PYZus{}epochs} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1} 

    \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{threshold}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}Predict the class label for each point in X}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{:}
            \PY{n}{X} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
        \PY{n+nb}{sum} \PY{o}{=} \PY{n}{X} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}
        \PY{k}{return} \PY{p}{(}\PY{n+nb}{sum} \PY{o}{\PYZgt{}} \PY{n}{threshold}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{predict\PYZus{}probability}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
       
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{:}
            \PY{n}{X} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{logistic}\PY{p}{(}\PY{n}{X} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{logistic}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{}Sigmoid function}
        \PY{k}{return} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{add\PYZus{}bias}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{bias}\PY{p}{)}\PY{p}{:}
        
        \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{n}{bias}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{442}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}data to be printed out :}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{} traning wtihout scaling data \PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{logCl} \PY{o}{=} \PY{n}{NumpyLogRegClass}\PY{p}{(}\PY{p}{)}
\PY{n}{logCl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{n}{t2\PYZus{}val}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)} \PY{c+c1}{\PYZsh{}train unscaled data}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of epochs trained:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{trained\PYZus{}epochs}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final loss on the training set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{losses\PYZus{}train}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZhy{}1 is last element in nparray}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final accuracy on the training set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{accuracies\PYZus{}train}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final loss on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{losses\PYZus{}val}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final accuracy on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{accuracies\PYZus{}val}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{logCl}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
--- traning wtihout scaling data ---
Number of epochs trained: 75
Final loss on the training set: 0.5275421073718891
Final accuracy on the training set: 0.72
Final loss on the validation set: 0.4929526398350684
Final accuracy on the validation set: 0.758
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{443}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{} traning on scaled data (with no specified val data ) \PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}some output us is infinity because of no val data set}

\PY{n}{logCl} \PY{o}{=} \PY{n}{NumpyLogRegClass}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}remaking object}
\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train\PYZus{}nm} \PY{o}{=} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t2\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{}scaling data}
\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{o}{=} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}

\PY{n}{logCl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{n}{t2\PYZus{}val}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of epochs trained:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{trained\PYZus{}epochs}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final loss on the training set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{losses\PYZus{}train}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final accuracy on the training set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{accuracies\PYZus{}train}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final loss on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{losses\PYZus{}val}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final accuracy on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{accuracies\PYZus{}val}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{}my plot does not seem right compared to the other output data like accuracy, i am unsure why this happens, maybe i did not scale the data properly?}
\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{logCl}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
--- traning on scaled data (with no specified val data ) ---
Number of epochs trained: 100
Final loss on the training set: 0.5281089145196718
Final accuracy on the training set: 0.719
Final loss on the validation set: 0.49698696995161545
Final accuracy on the validation set: 0.764
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{444}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}g) After a succesful training, for your best model,}
\PY{l+s+sd}{plot both training loss and validation loss as functions of the number of epochs in one figure,}
\PY{l+s+sd}{and both training and validation accuracies as functions of the number of epochs in another figure. }
\PY{l+s+sd}{Comment on what you see. Are the curves monotone? Is this as expected? : \PYZdq{}\PYZdq{}\PYZdq{}}

\PY{k}{def} \PY{n+nf}{plot\PYZus{}loss\PYZus{}and\PYZus{}accuracy}\PY{p}{(}\PY{n}{train\PYZus{}losses}\PY{p}{,} \PY{n}{val\PYZus{}losses}\PY{p}{,} \PY{n}{train\PYZus{}accuracies}\PY{p}{,} \PY{n}{val\PYZus{}accuracies}\PY{p}{)}\PY{p}{:}
    \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}losses}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plotting losses}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{train\PYZus{}losses}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}losses}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training and Validation Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plotting accuracies}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{train\PYZus{}accuracies}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}accuracies}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training and Validation Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} not using scaling of data}
\PY{n}{plot\PYZus{}loss\PYZus{}and\PYZus{}accuracy}\PY{p}{(}\PY{n}{logCl}\PY{o}{.}\PY{n}{losses\PYZus{}train}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{losses\PYZus{}val}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{accuracies\PYZus{}train}\PY{p}{,} \PY{n}{logCl}\PY{o}{.}\PY{n}{accuracies\PYZus{}val}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The loss curve is monotone while the accuracy curve is sometimes flat.
It was not expected to me that this would be the case. It seems the
Accuracy curve is flat even when the loss function curve is decreasing.

    \hypertarget{multi-class-classifiers}{%
\subsection{Multi-class classifiers}\label{multi-class-classifiers}}

We turn to the task of classifying when there are more than two classes,
and the task is to ascribe one class to each input. We will now use the
set (X, t\_multi).

    \hypertarget{one-vs-rest-with-logistic-regression}{%
\subsubsection{``One-vs-rest'' with logistic
regression}\label{one-vs-rest-with-logistic-regression}}

We saw in the lectures how a logistic regression classifier can be
turned into a multi-class classifier using the one-vs-rest approach. We
train one logistic regression classifier for each class. To predict the
class of an item, we run all the binary classifiers and collect the
probability score from each of them. We assign the class which ascribes
the highest probability.

Build such a classifier. Train the resulting classifier on (X\_train,
t\_multi\_train), test it on (X\_val, t\_multi\_val), tune the
hyper-parameters and report the accuracy.

Also plot the decision boundaries for your best classifier similarly to
the plots for the binary case.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{445}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{k}{class} \PY{n+nc}{OneVsRestLogisticRegression}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}\PY{p}{:}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{n}{learning\PYZus{}rate}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{n}{num\PYZus{}epochs}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tol}\PY{o}{=} \PY{n}{tol}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{models} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}multi\PYZus{}train}\PY{p}{)}\PY{p}{:}
        
        \PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{t\PYZus{}multi\PYZus{}train}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
        
        \PY{k}{for} \PY{n}{class\PYZus{}label} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}classes}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{}for labels in classes}
            
            \PY{n}{binaryLabelsMatrix} \PY{o}{=} \PY{p}{(}\PY{n}{t\PYZus{}multi\PYZus{}train} \PY{o}{==} \PY{n}{class\PYZus{}label}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)} \PY{c+c1}{\PYZsh{}compare, booleans represented im matrix by 0 and 1}
            \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}make new weights for ebery iteration}
            
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
                
                \PY{n+nb}{sum} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{weights}\PY{p}{)} \PY{c+c1}{\PYZsh{} X @ W did not work for some reason but np.dot() did work}
                \PY{n}{activation} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{)}
            
                \PY{n}{gradient} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{p}{(}\PY{n}{activation} \PY{o}{\PYZhy{}} \PY{n}{binaryLabelsMatrix}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} 
                \PY{c+c1}{\PYZsh{}update weights}
                \PY{n}{weights} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{gradient} 
                
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{weights}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{predict\PYZus{}probability}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
        
        \PY{n}{probabilities} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{models}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
        
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{models}\PY{p}{)}\PY{p}{)}\PY{p}{:}  \PY{c+c1}{\PYZsh{} Iterate each model}
            \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{models}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} 
            \PY{n}{probabilities}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}  \PY{c+c1}{\PYZsh{}apply activation and store in matrix collumn i }
        \PY{k}{return} \PY{n}{probabilities}  

    \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
        \PY{n}{probabilities} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict\PYZus{}probability}\PY{p}{(}\PY{n}{X}\PY{p}{)}
        \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{probabilities}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}argmax finding biggest value in the probability matrix}

\PY{c+c1}{\PYZsh{}i was unable to get better than 48\PYZpc{} accuracy}
\PY{n}{oneVrest} \PY{o}{=} \PY{n}{OneVsRestLogisticRegression}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}

\PY{n}{oneVrest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}multi\PYZus{}train}\PY{p}{)}

\PY{n}{predictions} \PY{o}{=} \PY{n}{oneVrest}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}

\PY{n}{accurac}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{predictions} \PY{o}{==} \PY{n}{t\PYZus{}multi\PYZus{}val}\PY{p}{)}\PY{c+c1}{\PYZsh{} neeed different var name because of scope issues}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accurac}\PY{p}{)}

\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}multi\PYZus{}val}\PY{p}{,} \PY{n}{oneVrest}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.482
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{for-in4050-students-multinomial-logistic-regression}{%
\subsubsection{For IN4050 students: Multinomial logistic
regression}\label{for-in4050-students-multinomial-logistic-regression}}

The following part is only mandatory for IN4050 students. IN3050
students are also welcome to make it a try. Everybody has to do the part
2 on multi-layer neural networks.

In the lectures, we contrasted the one-vs-rest approach with the
multinomial logistic regression, also called softmax classifier.
Implement also this classifier, tune the parameters, and compare the
results to the one-vs-rest classifier. (Don't expect a large difference
on a simple task like this.)

Remember that this classifier uses softmax in the forward phase. For
loss, it uses categorical cross-entropy loss. The loss has a somewhat
simpler form than in the binary case. To calculate the gradient is a
little more complicated. The actual gradient and update rule is simple,
however, as long as you have calculated the forward values correctly.

    \hypertarget{part-2-multi-layer-neural-networks}{%
\section{Part 2: Multi-layer neural
networks}\label{part-2-multi-layer-neural-networks}}

    \hypertarget{a-first-non-linear-classifier}{%
\subsection{A first non-linear
classifier}\label{a-first-non-linear-classifier}}

    The following code is a simple implementation of a multi-layer
perceptron or feed-forward neural network. For now, it is quite
restricted. There is only one hidden layer. It can only handle binary
classification. In addition, it uses a simple final layer similar to the
linear regression classifier above. One way to look at it is what
happens when we add a hidden layer to the linear regression classifier.

    The MLP class below misses the implementation of the \texttt{forward()}
function. Your first task is to implement it.

Remember that in the forward pass, we ``feed'' the input to the model,
the model processes it and produces the output. The function should make
use of the logistic activation function and bias.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{446}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} First, we define the logistic function and its derivative:}
\PY{k}{def} \PY{n+nf}{logistic}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{logistic\PYZus{}diff}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{y} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{447}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{MLPBinaryLinRegClass}\PY{p}{(}\PY{n}{NumpyClassifier}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}A multi\PYZhy{}layer neural network with one hidden layer\PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dim\PYZus{}hidden} \PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Intialize the hyperparameters\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{bias}
        \PY{c+c1}{\PYZsh{} Dimensionality of the hidden layer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim\PYZus{}hidden} \PY{o}{=} \PY{n}{dim\PYZus{}hidden}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activ} \PY{o}{=} \PY{n}{logistic}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activ\PYZus{}diff} \PY{o}{=} \PY{n}{logistic\PYZus{}diff}
        
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Perform one forward step. }
\PY{l+s+sd}{        Return a pair consisting of the outputs of the hidden\PYZus{}layer}
\PY{l+s+sd}{        and the outputs on the final layer\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Calculate the output of the hidden layer}
        \PY{n}{hidden\PYZus{}inputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights1}\PY{p}{)}
        \PY{n}{hidden\PYZus{}outputs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activ}\PY{p}{(}\PY{n}{hidden\PYZus{}inputs}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Add bias to the hidden layer outputs}
        \PY{n}{hidden\PYZus{}outputs\PYZus{}bias} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{n}{hidden\PYZus{}outputs}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Calculate the output of the final layer}
        \PY{n}{final\PYZus{}inputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{hidden\PYZus{}outputs\PYZus{}bias}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights2}\PY{p}{)}
        \PY{n}{final\PYZus{}outputs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activ}\PY{p}{(}\PY{n}{final\PYZus{}inputs}\PY{p}{)}
        
        \PY{k}{return} \PY{n}{hidden\PYZus{}outputs\PYZus{}bias}\PY{p}{,} \PY{n}{final\PYZus{}outputs}

            
    \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.1} \PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Intialize the weights. Train *epochs* many epochs.}
\PY{l+s+sd}{        }
\PY{l+s+sd}{        X\PYZus{}train is a NxM matrix, N data points, M features}
\PY{l+s+sd}{        t\PYZus{}train is a vector of length N of targets values for the training data, }
\PY{l+s+sd}{        where the values are 0 or 1.}
\PY{l+s+sd}{        lr is the learning rate}
\PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr} \PY{o}{=} \PY{n}{lr}
        
        \PY{c+c1}{\PYZsh{} Turn t\PYZus{}train into a column vector, a N*1 matrix:}
        \PY{n}{T\PYZus{}train} \PY{o}{=} \PY{n}{t\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
            
        \PY{n}{dim\PYZus{}in} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} 
        \PY{n}{dim\PYZus{}out} \PY{o}{=} \PY{n}{T\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Initialize the weights}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights1} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}
            \PY{n}{dim\PYZus{}in} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} 
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim\PYZus{}hidden}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{dim\PYZus{}in}\PY{p}{)}
        
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights2} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim\PYZus{}hidden}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} 
            \PY{n}{dim\PYZus{}out}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim\PYZus{}hidden}\PY{p}{)}
        \PY{n}{X\PYZus{}train\PYZus{}bias} \PY{o}{=} \PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Storage for loss and accuracy}
        \PY{n}{losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{accuracies} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{val\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{val\PYZus{}accuracies} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
           \PY{c+c1}{\PYZsh{}for tol stop part}
        \PY{n}{best\PYZus{}loss} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{epochs\PYZus{}no\PYZus{}update} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trained\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{0}

        \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} One epoch}
            \PY{c+c1}{\PYZsh{} The forward step:}
            \PY{n}{hidden\PYZus{}outs}\PY{p}{,} \PY{n}{outputs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bias}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} The delta term on the output node:}
            \PY{n}{out\PYZus{}deltas} \PY{o}{=} \PY{p}{(}\PY{n}{outputs} \PY{o}{\PYZhy{}} \PY{n}{T\PYZus{}train}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} The delta terms at the output of the hidden layer:}
            \PY{n}{hiddenout\PYZus{}diffs} \PY{o}{=} \PY{n}{out\PYZus{}deltas} \PY{o}{@} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights2}\PY{o}{.}\PY{n}{T}
            \PY{c+c1}{\PYZsh{} The deltas at the input to the hidden layer:}
            \PY{n}{hiddenact\PYZus{}deltas} \PY{o}{=} \PY{p}{(}\PY{n}{hiddenout\PYZus{}diffs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{*} 
                                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activ\PYZus{}diff}\PY{p}{(}\PY{n}{hidden\PYZus{}outs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}  

            
            \PY{c+c1}{\PYZsh{} Update the weights:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights2} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr} \PY{o}{*} \PY{n}{hidden\PYZus{}outs}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{out\PYZus{}deltas}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights1} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr} \PY{o}{*} \PY{n}{X\PYZus{}train\PYZus{}bias}\PY{o}{.}\PY{n}{T} \PY{o}{@} \PY{n}{hiddenact\PYZus{}deltas} 
    
            \PY{c+c1}{\PYZsh{} Calculate loss and accuracy}
            \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{outputs} \PY{o}{\PYZhy{}} \PY{n}{t\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}
            \PY{n}{accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{outputs} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)} \PY{o}{==} \PY{n}{t\PYZus{}train}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Store loss and accuracy}
            \PY{n}{losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
            \PY{n}{accuracies}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}

     

         \PY{c+c1}{\PYZsh{} Calculate loss and accuracy for validation set if provided}
            \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{o+ow}{and} \PY{n}{t\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{n}{val\PYZus{}hidden\PYZus{}outs}\PY{p}{,} \PY{n}{val\PYZus{}outputs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}\PY{p}{)}
                
                \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{val\PYZus{}outputs} \PY{o}{\PYZhy{}} \PY{n}{t\PYZus{}val}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}
                
                \PY{n}{val\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{val\PYZus{}outputs} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)} \PY{o}{==} \PY{n}{t\PYZus{}val}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} Store loss and accuracy for validation set}
                \PY{n}{val\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}
                \PY{n}{val\PYZus{}accuracies}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}accuracy}\PY{p}{)}

                \PY{c+c1}{\PYZsh{} Check for early stopping based on validation loss improvement}
                \PY{k}{if} \PY{n}{val\PYZus{}loss} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}loss} \PY{o}{\PYZhy{}} \PY{n}{tol}\PY{p}{:}
                    \PY{n}{best\PYZus{}loss} \PY{o}{=} \PY{n}{val\PYZus{}loss}
                    \PY{n}{epochs\PYZus{}no\PYZus{}update} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{epochs\PYZus{}no\PYZus{}update} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                    \PY{k}{if} \PY{n}{epochs\PYZus{}no\PYZus{}update} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{p}{:}
                        \PY{k}{break}


        \PY{c+c1}{\PYZsh{} Store losses and accuracies for inspection}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{losses} \PY{o}{=} \PY{n}{losses}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{accuracies} \PY{o}{=} \PY{n}{accuracies}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}losses} \PY{o}{=} \PY{n}{losses}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}accuracies} \PY{o}{=} \PY{n}{accuracies}
        
        \PY{k}{if} \PY{n}{X\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{o+ow}{and} \PY{n}{t\PYZus{}val} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}losses} \PY{o}{=} \PY{n}{val\PYZus{}losses}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}accuracies} \PY{o}{=} \PY{n}{val\PYZus{}accuracies}

        \PY{c+c1}{\PYZsh{} Store number of epochs trained}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trained\PYZus{}epochs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trained\PYZus{}epochs} \PY{o}{+} \PY{l+m+mi}{1}
        
    \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
\PY{+w}{        }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Predict the class for the members of X\PYZdq{}\PYZdq{}\PYZdq{}} 
        \PY{n}{Z} \PY{o}{=} \PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
        \PY{n}{forward} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{Z}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{score}\PY{o}{=} \PY{n}{forward}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
        \PY{k}{return} \PY{p}{(}\PY{n}{score} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.5}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{predict\PYZus{}probability}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}predicting probability of being of positive class}
        \PY{n}{Z} \PY{o}{=} \PY{n}{add\PYZus{}bias}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{Z}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
        
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{448}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{mlp} \PY{o}{=} \PY{n}{MLPBinaryLinRegClass}\PY{p}{(}\PY{p}{)}
\PY{n}{mlp}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{mlp}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{(}\PY{n}{mlp}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{}print(\PYZdq{}positiv class probability: \PYZdq{}, str(mlp.predict\PYZus{}probability(X\PYZus{}val)))}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on the validation set: 0.824
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{449}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}scaling data }
\PY{n}{mlp} \PY{o}{=} \PY{n}{MLPBinaryLinRegClass}\PY{p}{(}\PY{p}{)}
\PY{n}{ns} \PY{o}{=} \PY{n}{normalizerScalar}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train\PYZus{}nm} \PY{o}{=} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}val\PYZus{}nm}\PY{o}{=} \PY{n}{nS}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{ns}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{c+c1}{\PYZsh{}mlp.fit(X\PYZus{}train\PYZus{}nm, t2\PYZus{}train\PYZus{}nm,lr=0.1, epochs=1000)}
\PY{n}{mlp}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{,}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy on the validation set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{accuracy}\PY{p}{(}\PY{n}{mlp}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}nm}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{t2\PYZus{}train\PYZus{}nm}\PY{p}{,} \PY{n}{mlp}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy on the validation set: 0.756
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_46_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    When implemented, this model can be used to make a non-linear classifier
for the set \texttt{(X,\ t2)}. Experiment with settings for learning
rate and epochs and see how good results you can get. Report results for
various settings. Be prepared to train for a long time (but you can
control it via the number of epochs and hidden size).

Plot the training set together with the decision regions as in Part I.

    \hypertarget{improving-the-mlp-classifier}{%
\section{Improving the MLP
classifier}\label{improving-the-mlp-classifier}}

You should now make changes to the classifier similarly to what you did
with the logistic regression classifier in part 1.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  In addition to the \texttt{predict()} method, which predicts a class
  for the data, include the \texttt{predict\_probability()} method which
  predict the probability of the data belonging to the positive class.
  The training should be based on these values, as with logistic
  regression.
\item
  Calculate the loss and the accuracy after each epoch and store them
  for inspection after training.
\item
  Extend the \texttt{fit()} method with optional arguments for a
  validation set \texttt{(X\_val,\ t\_val)}. If a validation set is
  included in the call to \texttt{fit()}, calculate the loss and the
  accuracy for the validation set after each epoch.
\item
  Extend the \texttt{fit()} method with two keyword arguments,
  \texttt{tol} (tolerance) and \texttt{n\_epochs\_no\_update} and stop
  training when the loss has not improved for more than \texttt{tol}
  after \texttt{n\_epochs\_no\_update}. A possible default value for
  \texttt{n\_epochs\_no\_update} is 5. Add an attribute to the
  classifier which tells us after fitting how many epochs it was trained
  on.
\item
  Tune the hyper-parameters: \texttt{lr}, \texttt{tol} and
  \texttt{dim-hidden} (size of the hidden layer). Also, consider the
  effect of scaling the data.
\item
  After a succesful training with the best setting for the
  hyper-parameters, plot both training loss and validation loss as
  functions of the number of epochs in one figure, and both training and
  validation accuracies as functions of the number of epochs in another
  figure. Comment on what you see.
\item
  The MLP algorithm contains an element of non-determinism. Hence, train
  the classifier 10 times with the optimal hyper-parameters and report
  the mean and standard deviation of the accuracies over the 10 runs.
\end{enumerate}

    My answears:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  to d) solutions is in the code above.
\item
  It seems that a high tolerance (it seems high to me) gives
  consistently better results. Changing the other parameters didnt seem
  to help the mlp improve to much as the tolerance. Overall i got pretty
  inconsistent results. E
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{450}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}f)}
\PY{c+c1}{\PYZsh{}training with both val and train data}
\PY{n}{mlp}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{n}{t2\PYZus{}val}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}

\PY{n}{plot\PYZus{}loss\PYZus{}and\PYZus{}accuracy}\PY{p}{(}\PY{n}{mlp}\PY{o}{.}\PY{n}{losses}\PY{p}{,} \PY{n}{mlp}\PY{o}{.}\PY{n}{val\PYZus{}losses}\PY{p}{,} \PY{n}{mlp}\PY{o}{.}\PY{n}{accuracies}\PY{p}{,} \PY{n}{mlp}\PY{o}{.}\PY{n}{val\PYZus{}accuracies}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    With some parameters the graph looks pretty messy, these are some of the
nicer graphs i managed to get. the training and validation lines seems
similiar which is a good thing. ith certain other parameters however,
they did not look uniform at all.

    \begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  The MLP algorithm contains an element of non-determinism. Hence, train
  the classifier 10 times with the optimal hyper-parameters and report
  the mean and standard deviation of the accuracies over the 10 runs.
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{451}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{train\PYZus{}classifier\PYZus{}10\PYZus{}times}\PY{p}{(}\PY{n}{classifier}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{accuracies} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
        \PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t\PYZus{}train}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{n}{t\PYZus{}val}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{n}{tol}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{p}{)}
        \PY{n}{accuracy} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)} \PY{o}{==} \PY{n}{t\PYZus{}val}
        \PY{n}{accuracies}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{k}{return} \PY{n}{accuracies}

\PY{c+c1}{\PYZsh{}training 10 times }
\PY{n}{accuracies} \PY{o}{=} \PY{n}{train\PYZus{}classifier\PYZus{}10\PYZus{}times}\PY{p}{(}\PY{n}{mlp}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}

\PY{c+c1}{\PYZsh{}mean and std}
\PY{n}{mean\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{accuracies}\PY{p}{)}
\PY{n}{std\PYZus{}dev\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{accuracies}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{mean\PYZus{}accuracy}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard Deviation of Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{std\PYZus{}dev\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Mean Accuracy: 0.8226
Standard Deviation of Accuracy: 0.3820068585771727
    \end{Verbatim}

    \hypertarget{for-in4050-students-multi-class-neural-network}{%
\subsection{For IN4050-students: Multi-class neural
network}\label{for-in4050-students-multi-class-neural-network}}

    The following part is only mandatory for IN4050 students. IN3050
students are also welcome to make it a try. This is the most fun part of
the set :) )

The goal is to use a feed-forward neural network for non-linear
multi-class classfication and apply it to the set
\texttt{(X,\ t\_multi)}.

Modify the network to become a multi-class classifier. As a sanity check
of your implementation, you may apply it to \texttt{(X,\ t\_2)} and see
whether you get similar results as above.

Train the resulting classifier on \texttt{(X\_train,\ t\_multi\_train)},
test it on \texttt{(X\_val,\ t\_multi\_val)}, tune the hyper-parameters
and report the accuracy.

Plot the decision boundaries for your best classifier.

    \hypertarget{part-iii-final-testing}{%
\section{Part III: Final testing}\label{part-iii-final-testing}}

We can now perform a final testing on the held-out test set we created
in the beginning.

\hypertarget{binary-task-x-t2}{%
\subsection{Binary task (X, t2)}\label{binary-task-x-t2}}

Consider the linear regression classifier, the logistic regression
classifier and the multi-layer network with the best settings you found.
Train each of them on the training set and evaluate on the held-out test
set, but also on the validation set and the training set. Report the
performance in a 3 by 3 table.

Comment on what you see. How do the three different algorithms compare?
Also, compare the results between the different dataset splits. In cases
like these, one might expect slightly inferior results on the held-out
test data compared to the validation and training data. Is this the
case?

Also report precision and recall for class 1.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{452}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}training each classifier on training set}
\PY{n}{cl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n}{logCl}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{=}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t\PYZus{}val}\PY{o}{=}\PY{n}{t2\PYZus{}val}\PY{p}{,} \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{n}{n\PYZus{}epochs\PYZus{}no\PYZus{}update}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)} 
\PY{n}{mlp}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{,}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}

\PY{c+c1}{\PYZsh{}making table}
\PY{n}{performance\PYZus{}table} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{}adding linear reggresion datasets to table:}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{cl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{cl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{cl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{}adding logistic reggresion datasets to table:}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{logCl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{logCl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{logCl}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{}adding linear reggresion datasets to table:}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{mlp}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{mlp}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}
\PY{n}{performance\PYZus{}table}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{mlp}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracies on training, validation and test set respectively, lineargregression in first row, then logisitc and lastly mlp}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{performance\PYZus{}table}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}for empty space }


\PY{k}{def} \PY{n+nf}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}true}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Calculate precision, recall, fscore, and support for class 1}
    \PY{c+c1}{\PYZsh{} method imported fro library}
    \PY{n}{precision}\PY{p}{,} \PY{n}{recall}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{j} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}fscore\PYZus{}support}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{precision}\PY{p}{,} \PY{n}{recall}

\PY{c+c1}{\PYZsh{}calculate precision and recall for class 1 }
\PY{c+c1}{\PYZsh{}linear}
\PY{n}{precision\PYZus{}cl\PYZus{}train}\PY{p}{,} \PY{n}{recall\PYZus{}cl\PYZus{}train} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{cl}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{precision\PYZus{}cl\PYZus{}val}\PY{p}{,} \PY{n}{recall\PYZus{}cl\PYZus{}val} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{cl}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}
\PY{n}{precision\PYZus{}cl\PYZus{}test}\PY{p}{,} \PY{n}{recall\PYZus{}cl\PYZus{}test} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{cl}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{}logistic }
\PY{n}{precision\PYZus{}logCl\PYZus{}train}\PY{p}{,} \PY{n}{recall\PYZus{}logCl\PYZus{}train} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{logCl}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{precision\PYZus{}logCl\PYZus{}val}\PY{p}{,} \PY{n}{recall\PYZus{}logCl\PYZus{}val} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{logCl}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}
\PY{n}{precision\PYZus{}logCl\PYZus{}test}\PY{p}{,} \PY{n}{recall\PYZus{}logCl\PYZus{}test} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{logCl}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{}mlp}
\PY{n}{precision\PYZus{}mlp\PYZus{}train}\PY{p}{,} \PY{n}{recall\PYZus{}mlp\PYZus{}train} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{mlp}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{t2\PYZus{}train}\PY{p}{)}
\PY{n}{precision\PYZus{}mlp\PYZus{}val}\PY{p}{,} \PY{n}{recall\PYZus{}mlp\PYZus{}val} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{mlp}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{t2\PYZus{}val}\PY{p}{)}
\PY{n}{precision\PYZus{}mlp\PYZus{}test}\PY{p}{,} \PY{n}{recall\PYZus{}mlp\PYZus{}test} \PY{o}{=} \PY{n}{precision\PYZus{}recall\PYZus{}class1}\PY{p}{(}\PY{n}{mlp}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{t2\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print precision and recall for class 1}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision and recall for class 1 (positive class) on training, validation, and test sets respectively:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear tegression training:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}cl\PYZus{}train}\PY{p}{,} \PY{n}{recall\PYZus{}cl\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear tegression validation:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}cl\PYZus{}val}\PY{p}{,} \PY{n}{recall\PYZus{}cl\PYZus{}val}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear tegression test:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}cl\PYZus{}test}\PY{p}{,} \PY{n}{recall\PYZus{}cl\PYZus{}test}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic regression training:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}logCl\PYZus{}train}\PY{p}{,} \PY{n}{recall\PYZus{}logCl\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic regression validation:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}logCl\PYZus{}val}\PY{p}{,} \PY{n}{recall\PYZus{}logCl\PYZus{}val}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic regression test:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}logCl\PYZus{}test}\PY{p}{,} \PY{n}{recall\PYZus{}logCl\PYZus{}test}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MLP training:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}mlp\PYZus{}train}\PY{p}{,} \PY{n}{recall\PYZus{}mlp\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MLP validation:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}mlp\PYZus{}val}\PY{p}{,} \PY{n}{recall\PYZus{}mlp\PYZus{}val}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MLP test:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{precision\PYZus{}mlp\PYZus{}test}\PY{p}{,} \PY{n}{recall\PYZus{}mlp\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracies on training, validation and test set respectively, lineargregression
in first row, then logisitc and lastly mlp
[[0.721 0.76  0.72 ]
 [0.651 0.668 0.674]
 [0.788 0.828 0.788]]

Precision and recall for class 1 (positive class) on training, validation, and
test sets respectively:
Linear tegression training: 0.6575 0.6493827160493827
Linear tegression validation: 0.6857142857142857 0.7272727272727273
Linear tegression test: 0.6492146596858639 0.6294416243654822

Logistic regression training: 0.6284403669724771 0.33827160493827163
Logistic regression validation: 0.6454545454545455 0.35858585858585856
Logistic regression test: 0.6574074074074074 0.3604060913705584

MLP training: 0.7208237986270023 0.7777777777777778
MLP validation: 0.7456140350877193 0.8585858585858586
MLP test: 0.7136150234741784 0.7715736040609137
    \end{Verbatim}

    Comment: It seems the training sets have the best accuracies, compared
to the training and validation datasets. This is suprising to me since
the validation set usally in my tests did not do as well as the training
sets.

    \hypertarget{for-in4050-students-multi-class-task-x-t_multi}{%
\subsection{For IN4050-students: Multi-class task (X,
t\_multi)}\label{for-in4050-students-multi-class-task-x-t_multi}}

The following part is only mandatory for IN4050-students.
IN3050-students are also welcome to give it a try though.

Compare the three multi-class classifiers, the one-vs-rest and the
multinomial logistic regression from part one and the multi-class neural
network from part two. Evaluate on test, validation and training set as
above.

Comment on the results.

    Good luck!


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
